
<head>
<meta http-equiv="Content-Language" content="zh-cn">
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<title>VISSLAM</title>

<style>
<!--
div.Section1
	{page:Section1;}
 table.MsoNormalTable
	{mso-style-parent:"";
	font-size:10.0pt;
	font-family:"Times New Roman","serif"}
table.TableGrid
	{border:1.0pt solid black;
	font-size:10.0pt;
	font-family:"Times New Roman";
	}
-->
</style>
<meta http-equiv="Content-Language" content="zh-cn">
<meta http-equiv="Content-Type" content="text/html; charset=gb2312">
<title>DeepPS</title>

<style>
<!--
div.Section1
	{page:Section1;}
 table.MsoNormalTable
	{mso-style-parent:"";
	font-size:10.0pt;
	font-family:"Times New Roman","serif"}
table.TableGrid
	{border:1.0pt solid black;
	font-size:10.0pt;
	font-family:"Times New Roman";
	}
-->
</style>
</head>

<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns="http://www.w3.org/TR/REC-html40">

<body>

<table class="MsoNormalTable" border="0" cellpadding="0" width="1217" id="table3" height="35">
	<tr>
	<td valign="top" style="width: 1211px; height: 31px; padding: .75pt" align="left">
		
		<p class="text"><span lang="en-us"><font face="Calibri" size="5" color="#0000FF">
		<b>A Tightly-coupled Semantic SLAM System with Visual, Inertial and Surround-view Sensors for Autonomous Indoor Parking</b></font></span>
		
		<p class="text"><span lang="en-us"><font face="Calibri" size="4" color="#0000FF">
		Xuan Shao<sup>1</sup>,  Lin Zhang<sup>1</sup>,Tianjun Zhang<sup>1</sup>, Ying Shen<sup>1</sup>, Hongyu Li <sup>2</sup> and Yicong Zhou<sup>3</sup></font></span>

		<p class="text"><span lang="en-us"><font face="Calibri" size="4" color="#0000FF">
		<sup>1</sup>School of Software Engineering, Tongji University, Shanghai, China</font></span>
		<p class="text"><span lang="en-us"><font face="Calibri" size="4" color="#0000FF">
		<sup>2</sup>Tongdun AI Institute, Shanghai, China</font></span>
		<p class="text"><font face="Calibri" size="4" color="#0000FF"><sup>3</sup><span lang="en-us">
		Department of Computer and Information Science, University of Macau, Macau, China</span></font>
	</td>
	</tr>
</table>

<hr>
<p><span lang="en-us"><b><font face="Calibri" size="5">Introduction</font></b></span></p>
<p>
<span style="font-size: 13pt; font-family: Calibri; color: windowtext" lang="EN-US">
This is the website for our paper &quot;<a href="VISSLAM.pdf">A Tightly-coupled Semantic SLAM System with Visual, Inertial and Surround-view Sensors for Autonomous Indoor Parking<span style="color: #000000"></a>&quot;, 
ACM Multimedia 2020</span></span></p>

The semantic SLAM (simultaneous localization and mapping) system is an indispensable module for autonomous indoor parking. Monocular and binocular visual cameras constitute the basic configuration to build such a system. Features used in existing SLAM systems are often dynamically movable, blurred and repetitively textured. By contrast, semantic features on the ground are more stable and consistent in the indoor parking environment. Due to their inabilities to perceive salient features on the ground, existing SLAM systems are prone to tracking loss during navigation. Therefore, a surround-view camera system capturing images from a top-down viewpoint is necessarily called for. To this end, this paper proposes a novel tightly-coupled semantic SLAM system by integrating Visual, Inertial, and Surround-view sensors, VISSLAM for short, for autonomous indoor parking. In VISSLAM, apart from low-level visual features and IMU (inertial measurement unit) motion data, parking-slots in surround-view images are also detected and geometrically associated, forming semantic constraints. Specifically, each parking-slot can impose a surround-view constraint that can be split into an adjacency term and a registration term. The former
pre-defines the position of each individual parking-slot subject to whether it has an adjacent neighbor. The latter further constrains by registering between each observed parking-slot and its position in the world coordinate system.
<p align="left"><img border="0" src="abstract.png" height="458"></p>

<hr>
<p><span lang="en-us"><b><font face="Calibri" size="5">Overall Framwork</font></b></span></p>
<p>
<span lang="en-us"><font face="Calibri" style="font-size: 13pt">The overall framework of VISSLAM is shown in the following figure. Sensor configuration of VISSLAM consists of a front-view camera, an IMU and four fisheye cameras facing ground to form a surround-view camera system. Visual features from the front-view camera, preintegrated IMU measurements between two consecutive keyframes and parking-slots from the surround-view camera system constitute the multi-modal sensor data for VISSLAM. There are two major components in VISSLAM, sensor calibration and joint optimization. Sensor calibration is responsible for multi-modal sensor data fusion. The joint optimization model plays a critical role in tightly fusing multi-modal sensor measurements, which is the core of VISSLAM.</font></span></p>
<p align="left"><img border="0" src="Framework.png" height="458"></p>



<hr>
<p><span lang="en-us"><b><font face="Calibri" size="5">Revisiting Errors</font></b></span></p>
<p>
<span lang="en-us"><font face="Calibri" style="font-size: 13pt">Since it is difficult to obtain the ground truth of driving path, we can evaluate the localization accuracy by measuring the "revisiting error". Revisiting error is valid in localization
evaluation in SLAM system because an autonomous parking system allows for an absolute localization error during navigation. As long as the revisiting error is small enough, the vehicle will adopt a consistent driving strategy when it drives to the same position. In actual operation, the driver first manually drove the vehicle at around 10 km/h and the map was then initialized. Three map points at different locations were selected as reference points for test. Specifically, we chose two at the midpoints of both sides of the indoor parking site and one at the corner. After the map was stabilized (usually the vehicle should be driven for about three rounds), we evaluated by manually driving the vehicle to revisit three selected reference points, and recording the current coordinates at the test points. Then the differences in X-direction,Y-direction and Z-direction between the test points and reference points can be obtained. The final revisiting errors were computed by adding up errors in all directions. Revisiting errors on all three reference points (Point 1, Point 2, and Point 3) are presented below.</font></span></p>
<p align="left"><img border="0" src="revisiting_error.png"  height="300"></p>

<hr>
<p><span lang="en-us"><b><font face="Calibri" size="5">Tongji Underground Dataset & Relevant Codes </font></b></span></p>
<p>
<span lang="en-us"><font face="Calibri" style="font-size: 13pt">
<a href="https://baidu.com">Tongji Underground Dataset</a></font></span></p>
<p>
<span lang="en-us"><font face="Calibri" style="font-size: 13pt">The dataset provides synchronized front-view images and
surround-view images at 20 Hz and 10 Hz, respectively, with IMU measurements at 200 Hz. It contains 40,000+ front-view images and 20,000+ surround-view images, each of which was synthesized from four fisheye images, covering a wide variety of real cases in indoor parking sites. 10-dimensional motion data between every two consecutive front-view images was also collected by IMU. The resolutions of the fisheye camera and the front-view camera are 1280 * 1080 and 1280 * 720, respectively. The spatial resolution of each surround-view image is 416 * 416, corresponding to a 10m * 10m flat physical region, i.e., the length of 1 pixel in the surround-view image corresponds to 2.40cm on the physical ground.</font></span></p>
<p align="left"><img border="0" src="setup.png" height="458"></p>
<p align="left"><font face="Calibri" style="font-size: 13pt">This relevant code for processing the data can be found here
<a href="https://github.com">
https://github.com</a>. </font></p>

<hr>
<p><span lang="en-us"><b><font face="Calibri" size="5">Demo Videos</font></b></span></p>
<p>
<span lang="en-us"><font face="Calibri" style="font-size: 13pt">To qualitatively validate the effectiveness of the proposed VISSLAM, we drove the electric vehicle around an indoor parking site at around 10 km/h. The following is the demo video demonstrating the capability of our VISSLAM system.</font></span></p>
<video src="demo.mp4"  height="800" weight ="800"controls preload></video>
<hr>
<p align="justify"><font face="Calibri">Last update: <span lang="en-us">Aug. 3,</span>2020</span></font></p>
</body>
</html>